# modelFinetunnerFromPDF

This repository contains a Python-based pipeline for fine-tuning large language models (LLMs) and performing question answering based on custom datasets generated from PDF documents. The pipeline leverages the `unsloth` library for efficient model training and inference, incorporating techniques like 4-bit quantization for reduced memory usage.

## Overview

The core functionality of this project involves:

1.  **PDF Data Extraction and Dataset Generation:** The `getDataFromPdf.py` script extracts textual content from PDF files, which is then used to create a training dataset.
2.  **Language Model Fine-tuning:** The `fine_tune_pcs.py` script fine-tunes a pre-trained language model (specifically designed to work with `unsloth`) on the generated dataset. Users can specify a name for their fine-tuned model. If a model with the given name already exists, the fine-tuning step is skipped, and the existing model is loaded.
3.  **Interactive Inference:** The main script (`main.py`) loads the fine-tuned model and provides an interactive command-line interface where users can ask questions and receive answers generated by the model.

![ArchitectureModelTrainer(2)](https://github.com/user-attachments/assets/eb2f8b74-4370-40b7-b6f4-c448c16729be)



## File Structure
├── Dataset_Upload.py
├── fine_tune_pcs.py
├── getDataFromPdf.py
├── main.py
├── models/
└── rawdata/

### Key Files and Directories

* **`main.py`**: The primary entry point of the application. It orchestrates the dataset generation (if needed), model loading or fine-tuning, and the interactive inference session.
* **`fine_tune_pcs.py`**: Contains the `fine_tuning` function, which handles the process of loading a pre-trained model, preparing it for fine-tuning (likely using LoRA or similar techniques within `unsloth`), loading and preprocessing the training dataset, and initiating the training process using a trainer class (potentially from `transformers`).
* **`getDataFromPdf.py`**: Implements the logic to extract text data from PDF files located in the `rawdata/` directory. This extracted data is then formatted into a suitable training dataset.
* **`Dataset_Upload.py`**: (Its exact functionality isn't fully clear from the provided snippet, but it likely manages the uploading or handling of the training dataset.)
* **`models/`**: This directory is where the fine-tuned language models are saved. The subdirectory name will correspond to the model name provided by the user.
* **`rawdata/`**: This directory is intended to store the PDF files that will be processed by `getDataFromPdf.py` to generate the training dataset.

## Setup

### Prerequisites

* **Python 3.8+**
* **CUDA-enabled GPU** (recommended for efficient fine-tuning and inference)
* **Required Python Libraries**: Install the necessary libraries using pip:

    ```bash
    pip install unsloth transformers accelerate datasets bitsandbytes sentencepiece PyPDF2 # Ensure PyPDF2 or a similar PDF processing library is installed
    ```

### Installation

1.  **Clone the Repository** (if applicable):

    ```bash
    git clone https://github.com/MohansGopi/modelFinetunnerFromPDF
    cd modelFinetunnerFromPDF
    ```

2.  **Install Dependencies**: Install the required Python libraries:

    ```bash
    pip install -r requirements.txt # If you have a requirements.txt file
    # Otherwise, install the prerequisites manually as listed above.
    ```

3.  **Prepare PDF Data**: Place your PDF files in the `rawdata/` directory. The `getDataFromPdf.py` script will process these files.

## Usage

1.  **Run the Main Script**: Execute the `main.py` script:

    ```bash
    python main.py
    ```

2.  **Model Name Input**: You will be prompted to enter a name for your model.
    * If you enter a new name (or leave it blank, defaulting to `fine_tuned_llama_updated_ai_model`) and a directory with that name doesn't exist in the `models/` directory, the script will:
        * Process the PDF files in `rawdata/` using `getDataFromPdf.py` to create a training dataset.
        * Fine-tune a base language model using the `fine_tuning` function from `fine_tune_pcs.py` on the generated dataset. The fine-tuned model will be saved in `models/<your_model_name>/`.
    * If a directory with the specified model name already exists in `models/`, the script will load the pre-trained model from that directory, skipping the dataset generation and fine-tuning steps.

3.  **Interactive Querying**: Once the model is loaded, you will see the prompt `Enter your query :`. Type your question and press Enter. The model will generate a response based on the information it has learned.
    * To exit the interactive session, type `exit`.

### Example Workflow

1.  Place your PDF documents in the `rawdata/` folder.
2.  Run `python main.py`.
3.  Enter a model name (e.g., `my_qa_model`). If this is the first time, the script will generate a dataset and fine-tune a model with this name. Subsequent runs with the same name will load the existing model.
4.  Enter your questions at the prompt. The model will attempt to answer based on the content of your PDFs.
5.  Type `exit` to end the session.

## Code Overview

### `main.py`

* Determines whether to fine-tune a new model or load an existing one based on the user-provided model name and the presence of a corresponding directory in `models/`.
* Calls the `fine_tuning` function from `fine_tune_pcs.py` if a new model needs to be trained.
* Utilizes the `getDataFromPdf()` function (presumably from `getDataFromPdf.py`) to generate the training dataset from PDF files.
* Loads the fine-tuned model and tokenizer using `FastLanguageModel.from_pretrained` from the `unsloth` library.
* Implements the interactive inference loop using the `Gen_Json` function.
* The `Gen_Json` function takes a user query, formats it as a chat message, tokenizes it, generates a response using the loaded model, and then decodes the output to display the answer. It uses a temperature of `1.5` and `min_p` of `0.1` for controlling the generation process.

### `fine_tune_pcs.py`

* Contains the `fine_tuning` function, which is responsible for the model training process.
* This function likely loads a pre-trained base model and its tokenizer.
* It prepares the model for efficient fine-tuning, potentially using Low-Rank Adaptation (LoRA) as suggested by the `unsloth` library's capabilities.
* Loads and preprocesses the training dataset generated from the PDFs.
* Configures and uses a training class (likely from the `transformers` library, such as `SFTTrainer` for supervised fine-tuning) to train the model on the dataset.
* Saves the fine-tuned model and tokenizer to a directory named after the `modelName` in the `models/` directory.

### `getDataFromPdf.py`

* Contains the `getDataFromPdf()` function, which handles the extraction of text content from PDF files located in the `rawdata/` directory.
* It likely uses a library like `PyPDF2` (or similar) to parse the PDF files and extract the text.
* The extracted text is then processed and formatted into a dataset suitable for fine-tuning a language model, likely following a specific structure (e.g., question-answer pairs or conversational turns).

### `Dataset_Upload.py`

* (Based on the name, this script likely handles the uploading or management of datasets. Its internal workings are not detailed in the provided code.)

## Inference Configuration

The inference process in `main.py` uses the following configuration:

* `max_seq_length`: `2048` - Sets the maximum input sequence length for the model.
* `dtype`: `None` - Allows the library to automatically determine the appropriate data type.
* `load_in_4bit`: `True` - Loads the model weights in 4-bit precision to reduce memory footprint.
* `temperature`: `1.5` - Controls the randomness of the generated responses (higher values are more creative but can be less coherent).
* `min_p`: `0.1` - Used in nucleus sampling to filter out less probable tokens during generation.

## Notes and Potential Improvements

* **Dataset Formatting:** The specific format expected by the fine-tuning process for the dataset generated from PDFs is crucial. Ensure `getDataFromPdf.py` creates data in the correct format (e.g., conversational turns as implied by the chat template in `main.py`).
* **Error Handling:** Implementing more robust error handling (e.g., for file I/O operations, PDF parsing, and model loading) would improve the script's reliability.
* **Configuration Management:** Consider using a configuration file (e.g., `config.yaml` or `config.json`) to manage parameters like model names, data paths, and training hyperparameters.
* **Dataset Splitting:** For more rigorous training, consider splitting the generated dataset into training and validation sets.
* **Evaluation Metrics:** Implementing evaluation metrics to assess the performance of the fine-tuned model would be beneficial.
* **Progress Monitoring:** Adding progress bars or logging during the fine-tuning process can provide better feedback to the user.
* **Dependency Management:** Ensure all necessary libraries are correctly listed in a `requirements.txt` file for easier installation.
